---
title: "PS4 Andy Fan Will Sigal"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 

## Style Points (10 pts)

## Submission Steps (10 pts)

1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID):  Andy Fan, fanx
    - Partner 2 (name and cnet ID):
3. Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: 

AF    WS

5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: (Andy:0); (Will:0) Late coins left after submission: (Andy: 3) ; (Will: 4)
7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.
9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers. 


## Download and explore the Provider of Services (POS) file (10 pts)
(This file records all providers certified to bill Medicare and Medicaid. Each provider has a
 unique CMS certification number, consistent across different years)

(The data dictionary is located at the bottom of the page as “Provider of Services File- Hospital & Non-Hospital Facilities Data Dictionary)

```{python}
### SETUP 
import pandas as pd
import altair as alt
import time
import os
import warnings
import geopandas as gpd
import numpy as np
warnings.filterwarnings('ignore')

```

#### 1. (Partner 1) This is a fairly large dataset and we won’t be using most of the variables. Read through the rest of the problem set and look through the data dictionary to identify which variables you will need to complete the exercise, and use the tool on data.cms.gov into restrict to those variables (“Manage Columns”) before exporting (“Export”). Download this for 2016 and call it pos2016.csv. What are the variables you pulled?

required variables: 

provider type code:          PRVDR_CTGRY_CD

subtype code:                PRVDR_CTGRY_SBTYP_CD

CMS certification number:    PRVDR_NUM

Termination code:            PGM_TRMNTN_CD

zip code:                    ZIP_CD

Date of termination:         TRMNTN_EXPRTN_DT

Facility name:               FAC_NAME



```{python}
### import data (default directory is just the repo)

os.chdir('/Users/willsigal/Desktop') #will wd

#os.chdir('d:\\UChicago\\Classes\\2024Qfall\\Programming Python\\problem-set-4-andy-fan-will-sigal\\Data') #andy wd

df_2016 = pd.read_csv('pos2016.csv', encoding="latin1", on_bad_lines="skip")


```

#### 2. (Partner 1) Import your pos2016.csv file. We want to focus on short-term hospitals. These are identified as facilities with provider type code 01 and subtype code 01. Subset your data to these facilities. How many hospitals are reported in this data? Does this number make sense? Cross-reference with other sources and cite the number you compared it to. If it differs, why do you think it could differ?

```{python}
###subset to short term hositals
df_2016 = df_2016[(df_2016['PRVDR_CTGRY_SBTYP_CD'] == 1) & (df_2016['PRVDR_CTGRY_CD'] == 1)]
```

    a.There are 7275 short term hospitals reported in the data. This number at a glance makes sense nationwide.

    b.Looking at the Article by Jane Wishner, Patricia Solleveld, Robin Rudowitz, Julia Paradise, and Larisa Antonisse, they found "nearly 5,000 short-term, acute care hospitals in the United States". This is less than the data we have. It is possibly because some of the short-term hospitals in our dataset closed (they are temporary after all, and could have only been setup for less than a year)

#### 3. (Partner 1) Repeat the previous 3 steps with 2017Q4, 2018Q4, and 2019Q4 and then append them together. Plot the number of observations in your dataset by year.

```{python}
### import data 
df_2017 = pd.read_csv('pos2017.csv', encoding="latin1", on_bad_lines="skip")
df_2018 = pd.read_csv('pos2018.csv', encoding="latin1", on_bad_lines="skip")
df_2019 = pd.read_csv('pos2019.csv', encoding="latin1", on_bad_lines="skip")
```

```{python}
###subset to short term hositals
df_2017 = df_2017[(df_2017['PRVDR_CTGRY_SBTYP_CD'] == 1) & (df_2017['PRVDR_CTGRY_CD'] == 1)]
df_2018 = df_2018[(df_2018['PRVDR_CTGRY_SBTYP_CD'] == 1) & (df_2018['PRVDR_CTGRY_CD'] == 1)]
df_2019 = df_2019[(df_2019['PRVDR_CTGRY_SBTYP_CD'] == 1) & (df_2019['PRVDR_CTGRY_CD'] == 1)]
```

The number of short-term hospitals in each of the years:

2017: 7260 hospitals

2018: 7277 hospitals

2019: 7303 hospitals

```{python}
### create new year columns
df_2016['year'] = 2016
df_2017['year'] = 2017
df_2018['year'] = 2018
df_2019['year'] = 2019

### append
df = pd.concat([df_2016, df_2017, df_2018, df_2019], ignore_index=True)
```

```{python}
### altair plot
df1_3 = df.groupby('year').size().reset_index(name='count')

chartQ1_3 = alt.Chart(df1_3).mark_bar().encode(
    x= alt.X('year:O', axis=alt.Axis(labelAngle=90)),
    y= alt.Y('count:Q', title='number of hospitals', scale=alt.Scale(domain=[7000, 7500], clamp=True))
).properties(
    width=400, 
    height=200, 
    title='Observations of Hospitals by Year'  
)
chartQ1_3.save("chartQ1_3.png")
chartQ1_3
```

![Chart Q1.3](chartQ1_3.png)

#### 4. (Partner 1) Each hospital is identified by its CMS certification number. Plot the number of unique hospitals in your dataset per year. Compare this to your plot in the previous step. What does this tell you about the structure of the data?

    a.plot unique hospitals

```{python}
### plot
df1_4 = df.groupby('year')['PRVDR_NUM'].nunique().reset_index()

chartQ1_4 = alt.Chart(df1_4).mark_bar().encode(
    x= alt.X('year:O', axis=alt.Axis(labelAngle=90)),
    y= alt.Y('PRVDR_NUM:Q', title='number of unique hospitals', scale=alt.Scale(domain=[7000, 7500], clamp=True))
).properties(
    width=400, 
    height=200, 
    title='Unique Hospitals by Year'  
)
chartQ1_4.save("chartQ1_4.png")
chartQ1_4
```

![Chart Q1.4](chartQ1_4.png)

    b.compare to previous step, what does this tell about data structure:
    
The plots are identical. Which means each hospital is a unique observation, tallied each year.


## Identify hospital closures in POS file (15 pts) (*)

We will now use the 2016-2019 files to identify hospital closures nationally. A hospital issuspected to have closed if its Termination Code in the POS file lists them as an “Active Provider” in 2016 and then they are either not active or do not appear in the data at all in a subsequent year.

#### 1. (Partner 2) Use this definition to create a list of all hospitals that were active in 2016 that were suspected to have closed by 2019. Record the facility name and zip of each hospital as well as the year of suspected closure (when they become terminated or disappear from the data). How many hospitals are there that fit this definition?

#### 1. (Partner 2) Use this definition to create a list of all hospitals that were active in 2016 that were suspected to have closed by 2019. Record the facility name and zip of each hospital as well as the year of suspected closure (when they become terminated or disappear from the data). How many hospitals are there that fit this definition?

```{python}
active_2016 = df_2016[df_2016["PGM_TRMNTN_CD"] == 0]
active_2017 = df_2017[df_2017["PGM_TRMNTN_CD"] == 0]
active_2018 = df_2018[df_2018["PGM_TRMNTN_CD"] == 0]
active_2019 = df_2019[df_2019["PGM_TRMNTN_CD"] == 0]

def active_in_year(df, provider_num):
    row = df[df["PRVDR_NUM"] == provider_num]
    return not row.empty and row["PGM_TRMNTN_CD"].values[0] == 0

def determine_year_closed(provider_num):
    if not active_in_year(active_2017, provider_num):
        return 2017
    elif not active_in_year(active_2018, provider_num):
        return 2018
    elif not active_in_year(active_2019, provider_num):
        return 2019
    return None

active_2016_only = active_2016[active_2016["PGM_TRMNTN_CD"] == 0]
active_2016_only["Year_Closed"] = active_2016_only["PRVDR_NUM"].apply(determine_year_closed)

closed_hospitals = active_2016_only.dropna(subset=["Year_Closed"]).reset_index(drop=True)

print(f"Number of suspected hospital closures: {len(closed_hospitals)}")
print(closed_hospitals[["FAC_NAME", "PRVDR_NUM", "ZIP_CD", "Year_Closed"]].sort_values(by="FAC_NAME").head(10))
```



#### 2. (Partner 2) Sort this list of hospitals by name and report the names and year of suspected closure for the first 10 rows.
```{python}
first_ten_closures = closed_hospitals.sort_values(by='FAC_NAME').head(10)[['FAC_NAME', 'Year_Closed']]

first_ten_closures
```

#### 3. (Partner 2) However, not all suspected hospital closures are true closures. For example, in the case of a merger, a CMS certification number will be appear to be “terminated,” but then the hospital re-appear under a similar name/address with a new CMS certifi cation number in the next year. As a first pass to address this, remove any suspected hospital closures that are in zip codes where the number of active hospitals does not decrease in the year after the suspected closure.

    a. Among the suspected closures, how many hospitals fit this definition of potentially being a merger/acquisition?
```{python}
active_hospitals_count = df[df['PGM_TRMNTN_CD'] == 0].groupby(['ZIP_CD', 'year']).size().reset_index(name='active_count')

#count hospitals by zip each year
zip_year_counts = active_hospitals_count.pivot(index='ZIP_CD', columns='year', values='active_count').fillna(0)

def post_closure_counts(row):
    closure_year = row['Year_Closed']
    if row['ZIP_CD'] in zip_year_counts.index:
        # Get counts for years after the closure year
        post_years = zip_year_counts.columns[zip_year_counts.columns > closure_year]
        counts = zip_year_counts.loc[row['ZIP_CD'], post_years]
        return counts.values if not counts.empty else np.array([0])
    return np.array([0])

closed_hospitals['post_closure_counts'] = closed_hospitals.apply(post_closure_counts, axis=1)


#check if count is higher or equal in any years post closure
closed_hospitals['closure_year_count'] = closed_hospitals.apply(
    lambda row: zip_year_counts.at[row['ZIP_CD'], row['Year_Closed']]
    if (row['ZIP_CD'] in zip_year_counts.index and row['Year_Closed'] in zip_year_counts.columns) else 0, axis=1
)

potential_mergers = closed_hospitals[
    closed_hospitals.apply(lambda row: any(row['closure_year_count'] <= count for count in row['post_closure_counts']), axis=1)
]

merger_count = potential_mergers.shape[0]
print(f"Number of potential mergers: {merger_count}")
```

    b. After correcting for this, how many hospitals do you have left?
```{python}
true_closures = closed_hospitals[~closed_hospitals.index.isin(potential_mergers.index)]
remaining_closures_count = true_closures.shape[0]

remaining_closures_count
```

    c. Sort this list of corrected hospital closures by name and report the first 10 rows.
```{python}
correct_closure_df = true_closures.sort_values(by='FAC_NAME').head(10)[['FAC_NAME', 'ZIP_CD', 'Year_Closed']]

correct_closure_df.head(10)
```

## Download Census zip code shapefile (10 pt) 

Navigate to the Census shapefiles (link), select “gz_2010_us_860_00_500k.zip” and download the resulting shapefile. Note: If you have difficulty downloading or working with the file because the size is too large for your computer, reach out to the instruction team on Ed so that we can give them a smaller initial shapefile to work with.

#### 1. (Partner 1) This is non-tabular data. 1a. What are the five file types and what type of information is in each file? 1b. It will be useful going forward to have a sense going forward of which files are big versus small. After unzipping, how big is each of the datasets?

    a.The 5 file types are: 
    1.dbf file--which is a table that stores attributes of the geometries. 
    2.prj file--stores the coordinate system information; used by ArcGIS.
    3.shp file--The main file that stores the feature geometry (by far the largest, >800mb in this instance)
    4.shx file--the index file for the geometries
    5.xml file--is metadata for ArcGIS that has info about the shapefile itself

    b. The size of the files:
    1.dbf file--6275kb
    2.prj file--1kb
    3.shp file--817,915kb
    4.shx file--259kb
    5.xml file--16kb

#### 2. (Partner 1) Load the zip code shapefile and restrict to Texas zip codes. (Hint: you can identify which state a zip code is in using the first 2-3 numbers in the zip code (Wikipedia link). Then calculate the number of hospitals per zip code in 2016 based on the cleaned POS file from the previous step. Plot a choropleth of the number of hospitals by zip code in Texas.

```{python}
import geopandas as gpd
os.chdir('/Users/willsigal/Desktop/Shapefiles') #will wd shapefiles

#os.chdir('d:\\UChicago\\Classes\\2024Qfall\\Programming Python\\problem-set-4-andy-fan-will-sigal\\Shapefiles') #andy wd shapefiles

zips_all = gpd.read_file('gz_2010_us_860_00_500k.shp')
gpd_texas = pd.DataFrame(zips_all)
#texas zips: 75xxx-79xxx, 885xx, (also 73301 but for IRS only)
gpd_texas = gpd_texas[gpd_texas['ZCTA5'].str.startswith(('75','76','77','78','79','885'))]
```

```{python}
### number of hospitals per zip:
df_2016_tx = df_2016
df_2016_tx['ZIP_CD'] = df_2016_tx['ZIP_CD'].astype(int)
df_2016_tx['ZIP_CD'] = df_2016_tx['ZIP_CD'].astype(str)
df_2016_tx = df_2016_tx[df_2016_tx['ZIP_CD'].isin(gpd_texas['ZCTA5'])]

tx_zip = df_2016_tx['ZIP_CD'].value_counts().reindex(gpd_texas['ZCTA5']).fillna(0).astype(int)
tx_zip = tx_zip.reset_index()

#asked chatgpt how to find the number of observations for each zip code in df1 using observations from df2
```

```{python}
###Plot a choropleth of the number of hospitals by zip code in Texas.
#append count to gpd
zips_texas = zips_all[zips_all['ZCTA5'].str.startswith(('75','76','77','78','79','885'))]
zips_texas = pd.merge(zips_texas, tx_zip, on='ZCTA5', how='left')

#create area column in km2
zips_texas["area_km2"] = zips_texas.area / 1000000 
zips_texas[['ZCTA5', 'count', 'area_km2', 'geometry']].head()

zips_texas.plot(column="count", legend=True)
```

*ZCTA5 is the Census 5 digit zip code tabulation area, and will be considered the zip code as no other variable is avaliable.

## Calculate zip code’s distance to the nearest hospital (20 pts) (*)

#### 1. (Partner 2) Create a GeoDataFrame for the centroid of each zip code nationally: zips_all_centroids. What are the dimensions of the resulting GeoDataFrame and what do each of the columns mean?
```{python}
zips_all_centroids = zips_all
zips_all_centroids['geometry'] = zips_all['geometry'].centroid
zips_all_centroids.shape
```


```{python}
zips_all_centroids.columns
```

```{python}
zips_all_centroids.plot()
```

Geo_id is the unique identifier, ZCTA5 is the zip code, Name is the name of the area, censusarea is the area of the zcta in square kilometers and geometry is the centroid point geomotry for each ZCTA. There are 33,120 rows as there are 33,120 observations


#### 2. (Partner 2) Create two GeoDataFrames as subsets of zips_all_centroids. First, create all zip codes in Texas: zips_texas_centroids. Then, create all zip codes in Texas or a bordering state: zips_texas_borderstates_centroids, using the zip code prefixes to make these subsets. How many unique zip codes are in each of these subsets? 
```{python}
texas_prefixes = ['75', '76', '77', '78', '79']

border_states_prefixes = {'New Mexico': ['87', '88'],'Oklahoma': ['73', '74'],'Arkansas': ['71', '72'],'Louisiana': ['70', '71']}

zips_texas_centroids = zips_all_centroids[zips_all_centroids['ZCTA5'].str[:2].isin(texas_prefixes)]

all_prefixes = texas_prefixes + [prefix for state_prefixes in border_states_prefixes.values() for prefix in state_prefixes]

zips_texas_borderstates_centroids = zips_all_centroids[zips_all_centroids['ZCTA5'].str[:2].isin(all_prefixes)]

texas_unique_zips = zips_texas_centroids['ZCTA5'].nunique()
border_states_unique_zips = zips_texas_borderstates_centroids['ZCTA5'].nunique()


print(f'there are {texas_unique_zips} unique zipcodes in Texas')

print(f'there are {border_states_unique_zips} unique zips in Texas and bordering states')

```

#### 3.(Partner 2) Then create a subset of zips_texas_borderstates_centroids that contains only the zip codes with at least 1 hospital in 2016. Call the resulting GeoDataFrame zips_withhospital_centroids What kind of merge did you decide to do, and what variable are you merging on? 

```{python}
zips_texas_borderstates_centroids['ZCTA5'].head
```

```{python}
hospitals_2016 = df[(df['year'] == 2016) & (df['PGM_TRMNTN_CD'] == 0)]
hospitals_2016['ZIP_CD'] = hospitals_2016['ZIP_CD'].astype(int).astype(str)

zips_texas_borderstates_centroids['ZCTA5'] = zips_texas_borderstates_centroids['ZCTA5']

zip_codes_with_hospitals = hospitals_2016['ZIP_CD'].unique()

zips_withhospital_centroids = zips_texas_borderstates_centroids[zips_texas_borderstates_centroids['ZCTA5'].isin(zip_codes_with_hospitals)]

zips_withhospital_centroids.plot(markersize = 5)
```

```{python}
len(zips_withhospital_centroids)
```

I decided to first filter for active hospitals in the 2016 df, then I made the zipswithhospitals by filtering the zips_texas_border_state_centroid df zips by whether the zips are in the active hospital zip. 

#### 4. (Partner 2) For each zip code in zips_texas_centroids, calculate the distance to the nearest zip code with at least one hospital in zips_withhospital_centroids.

    a. This is a computationally-intensive join. Before attempting to do the entire join, subset to 10 zip codes in zips_texas_centroids and try the join. How long did it take? Approximately how long do you estimate the entire procedure will take?

```{python}
import time
from shapely import Point
subset_texas_centroids = zips_texas_centroids.head(10)

start_time = time.time()

distances = []
for _, texas_zip in subset_texas_centroids.iterrows():
    min_distance = zips_withhospital_centroids.distance(texas_zip.geometry).min()
    distances.append(min_distance)


end_time = time.time()

time_taken_subset = end_time - start_time
print(f'Time taken for 10 ZIP codes was {time_taken_subset:.2f} seconds')

full_data_estimate = (time_taken_subset / 10) * len(zips_texas_centroids)

print(f'it will take approx. {full_data_estimate} seconds to join the whole df')

```

    b. Now try doing the full calculation and time how long it takes. How close is it to your estimation?

```{python}
start_time_full = time.time()

full_distances = []
for _, texas_zip in zips_texas_centroids.iterrows():
    min_distance = zips_withhospital_centroids.distance(texas_zip.geometry).min()
    full_distances.append(min_distance)


end_time_full = time.time()

actual_time_full = end_time_full - start_time_full

print(f'the whole calculation took {actual_time_full} this is very close to my calculation')

zips_texas_centroids['nearest_hospital_distance'] = full_distances

```

    c. Look into the .prj file and report which unit it is in. Convert the given unit to miles, using an appropriate conversion you find online (estimates are okay).

```{python}
with open("gz_2010_us_860_00_500k.prj", 'r') as file:
    prj_text = file.read()

print(prj_text) ## units are in degrees

zips_texas_centroids['nearest_hospital_distance_miles'] = zips_texas_centroids['nearest_hospital_distance'] * 69

```

#### 5.(Partner 2) Calculate the average distance to the nearest hospital for each zip code in Texas. 

    a. What unit is this in
```{python}
#Its in degrees
```

    b. Report the average distance in miles. Does this value make sense?

```{python}
average_distance_miles = zips_texas_centroids['nearest_hospital_distance_miles'].mean()

print(f'The average distance is {average_distance_miles:.2f} miles.')

``` 

    c. Map the value for each zip code

```{python}
zips_texas_centroids['nearest_hospital_distance']
```

```{python}

zips_texas_centroids['nearest_hospital_distance_miles'] = pd.to_numeric(zips_texas_centroids['nearest_hospital_distance_miles'], errors='coerce')

zips_texas_centroids.plot(
    column='nearest_hospital_distance_miles', 
    cmap='coolwarm', markersize = 2,
    legend=True,legend_kwds={'label': "Distance to Nearest Hospital (Miles)"})
```

## Effects of closures on access in Texas (15 pts)

#### 1. (Partner 1) Using the corrected hospital closures dataset from the first section, create a list of directly affected zip codes in Texas– that is, those with at least one closure in 2016-2019. Display a table of the number of zip codes vs. the number of closures they experienced. 

```{python}
df5 = df[df['TRMNTN_EXPRTN_DT'].notna()]
df5['ZIP_CD'] = df5['ZIP_CD'].astype(int)
df5['ZIP_CD'] = df5['ZIP_CD'].astype(str)
Q5_1_closures = df5['ZIP_CD'].value_counts().rename("number_closures")
Q5_1_closures
```

#### 2.(Partner 1) Plot a choropleth of which Texas zip codes were directly affected by a closure in 2016-2019– there was at least one closure within the zip code. How many directly affected zip codes are there in Texas? 

```{python}
Q5_1_closures = Q5_1_closures.reset_index()
Q5_1_closures.columns = ['ZCTA5', 'number_closures']

zips_texas5_2 = zips_texas
zips_texas5_2 = zips_texas5_2.merge(Q5_1_closures, on='ZCTA5', how='left')
zips_texas5_2['number_closures'] = zips_texas5_2['number_closures'].fillna(0)
print((zips_texas5_2['number_closures'] > 0).sum())

###plot
zips_texas5_2.plot(column="number_closures", legend=True)
```

There are 334 Zip codes in Texas that were affected by at least 1 closure between 2017-2019

#### 3. (Partner 1) Then identify all the indirectly affected zip codes: Texas zip codes within a 10-mile radius of the directly affected zip codes. To do so, first create a GeoDataFrame of the directly affected zip codes. Then create a 10-mile buffer around them. Then, do a spatial join with the overall Texas zip code shapefile. How many indirectly affected zip codes are there in Texas? 

```{python}
import matplotlib.pyplot as plt
### gdf of directly affected zips
zips_texas5_3 = zips_texas5_2[zips_texas5_2['number_closures'] > 0]
### create buffer
zips_texas5_3.crs #check distance units.   ### 1 degree latitude =69 miles, therefore 1 mile =  1/69 degrees = 0.01449
zips_texas5_3['geometry'] = zips_texas5_3.geometry.buffer(0.01449)

###join
zips_texas5_3_joined = gpd.sjoin(zips_texas5_2, zips_texas5_3, how='inner')

zips_texas5_3_joined['ZCTA5_left'].nunique()

#asked chatgpt "after conducting a spacial left join, how to find number of overlapping areas, given a buffer on the right geometries"

```

There are 1518 total affected zip codes, and 1184 directly affected zip codes in Texas.

#### 4.(Partner 1) Make a choropleth plot of the Texas zip codes with a different color for each of the 3 categories: directly affected by a closure, within 10 miles of closure but not directly affected, or not affected. 

```{python}
#df_test = pd.DataFrame(zips_texas5_3_joined)
```

## Reflecting on the exercise (10 pts) 

#### (Partner 1) The “first-pass” method we’re using to address incorrectly identified closures in the data is imperfect. Can you think of some potential issues that could arise still and ways to do a better job at confirming hospital closures?

#### (Partner 2) Consider the way we are identifying zip codes affected by closures. How well does this reflect changes in zip-code-level access to hospitals? Can you think of some ways to improve this measure?
While identifying closures by zip codes might provide numbers of active hospitals in a zip code it does take into consideration population size of the area that the hospital was serving or anything to do with the health care capacity lost by the closure. We could improve our understanding of how closures impact zip-code level access to hospitals by adding metrics on the capcity of the hospitals lost, like number of beds, how long the drive is to the next nearest hospital or the hospital/population ratio before and after the closure. 